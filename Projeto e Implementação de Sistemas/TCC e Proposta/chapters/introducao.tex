\chapter{Introdução}
\label{c.introducao}

O tema deste trabalho é a exploração dos algoritmos clássicos de compressão de dados, abordando seus aspectos teóricos, implementação prática e análise de desempenho. A compressão de dados é uma técnica essencial na ciência da computação, utilizada para reduzir o tamanho dos arquivos, otimizando o uso de recursos e acelerando a transmissão de dados, o que se torna crucial na era digital em que vivemos~\cite{salomon2007data}.  

O desenvolvimento dessa área teve início na década de 1950, com os primeiros métodos que visavam otimizar a utilização do espaço de armazenamento. Entre esses métodos, destaca-se o algoritmo de Huffman, que utiliza uma técnica de codificação de prefixo variável. Nesse algoritmo, símbolos mais frequentes recebem códigos de comprimento menor, enquanto símbolos menos frequentes recebem códigos mais longos~\cite{salomon2007data}. O método de Huffman é amplamente empregado em compressão sem perdas e se tornou fundamental para a criação de arquivos compactados em formatos como ZIP e GZIP~\cite{deutsch1996gzip}. Sua simplicidade, aliada à sua eficácia, fez com que se consolidasse como um dos pilares da compressão de dados.  

Nos anos seguintes, com o aumento da demanda por transmissão de grandes volumes de dados, novos algoritmos surgiram. Um exemplo significativo é o LZ77 (Lempel-Ziv 77), introduzido por Abraham Lempel e Jacob Ziv em 1977~\cite{ziv1977universal}. O LZ77 aplica uma técnica de compressão baseada em dicionários, onde sequências de dados repetidas são substituídas por referências a posições anteriores. Essa abordagem inspirou a criação de outros algoritmos, como o LZW (Lempel-Ziv-Welch), que aprimora o LZ77 ao criar dinamicamente um dicionário de strings enquanto os dados são comprimidos~\cite{welch1984technique}. O LZW é particularmente famoso pela sua aplicação no formato de compressão GIF, bem como em arquivos TIFF.  

Além desses, o GZIP se destaca como outro algoritmo amplamente utilizado, especialmente em sistemas Unix e na web. O GZIP combina a codificação de Huffman com o algoritmo LZ77, permitindo uma compressão eficiente e rápida, sem perda de dados~\cite{deutsch1996gzip}. Sua popularidade decorre da combinação de alta taxa de compressão e facilidade de descompressão, sendo uma escolha recorrente em aplicações como transferência de arquivos e armazenamento de dados.  

Os métodos de Huffman, LZ77, LZW e GZIP formam a espinha dorsal dos algoritmos clássicos de compressão de dados e continuam sendo amplamente utilizados em diversas áreas, apesar do surgimento de novas abordagens.  

Atualmente, a compressão de dados continua a ser um campo dinâmico e em constante evolução, com diversos algoritmos sendo constantemente estudados e aprimorados. Entre os mais recentes, destacam-se os métodos baseados em compressão adaptativa, que ajustam seus parâmetros conforme as características dos dados a serem comprimidos, oferecendo maior eficiência dependendo do conteúdo~\cite{salomon2007data}. Um exemplo é o Brotli, um algoritmo de compressão sem perdas desenvolvido pela Google, amplamente utilizado para comprimir arquivos em navegadores web. O Brotli combina codificação de Huffman e transformações de fluxo de dados, permitindo taxas de compressão superiores às oferecidas por algoritmos tradicionais como o GZIP~\cite{alakuijala2016brotli}.  

Além disso, algoritmos de compressão em tempo real têm ganhado relevância devido ao crescente volume de dados gerados em tempo real, como no streaming de vídeo e nas comunicações móveis. O Zstandard (ou Zstd), desenvolvido pelo Facebook, é um exemplo notável. Ele oferece uma excelente combinação entre alta taxa de compressão e velocidade, podendo ser utilizado tanto em compressões em tempo real quanto em grandes volumes de dados~\cite{collet2016zstandard}. O Zstandard é considerado uma evolução do LZ4, sendo mais eficiente tanto na compressão quanto na descompressão.  

Em áreas como compressão de imagens e áudio, técnicas baseadas em aprendizado de máquina e redes neurais também têm
atraído atenção. No campo da compressão de imagens, os Modelos Generativos Adversariais (GANs) estão sendo explorados
para desenvolver algoritmos que geram representações comprimidas de alta qualidade, mantendo os detalhes visuais. Na
compressão de áudio, algoritmos baseados em redes neurais, como o WaveNet~\cite{wavenet}, estão sendo experimentados para aprimorar a qualidade da compressão, tanto com perdas quanto sem perdas.  

Outro campo de grande importância é a compressão de vídeo, especialmente para plataformas de streaming como Netflix e YouTube. Algoritmos como o HEVC (High Efficiency Video Coding), uma evolução do H.264, continuam a ser aprimorados para garantir uma compressão mais eficiente, sem perda significativa de qualidade. Além disso, o desenvolvimento de novos codecs como o VVC (Versatile Video Coding) está em andamento, com a promessa de oferecer compressão ainda mais eficiente para vídeos de alta definição e 4K.  

A análise de desempenho, que inclui a eficiência em termos de tempo de execução e taxa de compressão, também desempenha um papel crucial. Com o aumento exponencial da quantidade de dados gerados e transmitidos na sociedade moderna, os algoritmos de compressão tornaram-se ainda mais essenciais. A eficiência na compressão não só reduz os custos de armazenamento e acelera a transmissão de dados, mas também impacta diretamente áreas como streaming de vídeos, comunicação móvel, redes de computadores e sistemas de armazenamento em nuvem. Compreender os diferentes métodos e avaliar sua aplicabilidade em cenários diversos é fundamental para o desenvolvimento de soluções mais eficazes e eficientes~\cite{salomon2007data}.

