\chapter{Introdução}
\label{c.introducao}

%O tema deste trabalho é a exploração de algoritmos clássicos de compressão de dados, abordando seus aspectos teóricos, implementação prática e análise de desempenho. A compressão de dados é uma técnica fundamental na ciência da computação, utilizada para reduzir o tamanho dos arquivos, otimizando o uso de recursos e acelerando a transmissão de dados, o que se torna crucial na época em que vivemos.

%A área começou a se desenvolver na década de 1950, com os primeiros métodos que visavam otimizar a utilização do espaço de armazenamento, como o método de Huffman, um algoritmo baseado em uma técnica de codificação de prefixo variável, na qual símbolos mais frequentes recebem códigos de comprimento menor, enquanto símbolos menos frequentes recebem códigos mais longos. Esse algoritmo é utilizado principalmente em compressão sem perdas e é fundamental para a criação de arquivos compactados em formatos como ZIP e GZIP. Sua simplicidade e eficácia o tornaram um dos pilares da compressão de dados.

%Nos anos seguintes, com a crescente necessidade de transmitir grandes volumes de dados, surgiram novos algoritmos como o LZ77 (Lempel-Ziv 77), introduzido por Abraham Lempel e Jacob Ziv em 1977. O LZ77 utiliza uma técnica de compressão baseada em dicionários, onde sequências de dados repetidas são substituídas por referências a uma posição anterior. Essa abordagem é a base de muitos algoritmos subsequentes, como o LZW (Lempel-Ziv-Welch), que aprimora o LZ77 ao criar um dicionário dinâmico de strings à medida que os dados são comprimidos. O LZW é famoso por sua utilização no formato de compressão GIF e também em arquivos TIFF. 

%Além desses, o GZIP é outro algoritmo amplamente utilizado, especialmente para compressão de arquivos em sistemas Unix e na web. O GZIP combina a codificação de Huffman com o algoritmo LZ77, permitindo uma compressão eficiente e rápida, sem perda de dados. Sua popularidade se deve à combinação de boa taxa de compressão e a capacidade de ser descomprimido facilmente, o que o torna uma escolha recorrente em aplicações como transferência de arquivos e armazenamento de dados.

%O método de Huffman, LZ77, LZW e GZIP formam a espinha dorsal dos métodos de compressão de dados tradicionais e continuam a ser utilizados em várias áreas 

%Atualmente, a área de compressão de dados continua a ser um campo ativo de pesquisa, com diversos algoritmos sendo estudados e aprimorados. Entre os mais recentes e em destaque, estão os métodos baseados em compressão adaptativa, que ajustam seus parâmetros de acordo com os dados a serem comprimidos, oferecendo uma maior eficiência dependendo do tipo de conteúdo. Um exemplo disso é o Brotli, um algoritmo de compressão sem perdas desenvolvido pela Google que é amplamente utilizado para comprimir arquivos em navegadores web, devido à alta eficiência e baixa latência. Brotli utiliza um modelo de compressão baseado em uma combinação de codificação de Huffman e transformações de fluxo de dados, permitindo taxas de compressão superiores às oferecidas por outros algoritmos tradicionais, como o GZIP.

%Além disto, os algoritmos de compressão em tempo real estão ganhando importância devido ao aumento de dados gerados em tempo real, como os usados em streaming de vídeo e comunicações móveis. Algoritmos como o Zstandard (ou Zstd), desenvolvido pelo Facebook, são exemplos notáveis, pois oferecem uma excelente combinação entre alta taxa de compressão e velocidade, podendo ser usadas tanto para comprimir em tempo real quanto para grandes volumes de dados. O Zstandard é considerado uma evolução do LZ4, sendo mais eficiente em compressão e descompressão. 

%Em áreas como a compressão de imagens e áudio, técnicas baseadas em aprendizado de máquina e redes neurais também têm ganhado atenção. Por exemplo, no campo da compressão de imagens, os Modelos Generativos Adversariais (GANs) têm sido exploradas para desenvolver algoritmos que geram representações comprimidas de alta qualidade, mantendo detalhes visuais. Já na compressão de áudio, algoritmos baseados em redes neurais, como o WaveNet, estão sendo experimentados para melhorar a qualidade da compressão sem perdas e com perdas.

%Outro campo promissor é a compressão de vídeo, que é extremamente importante para plataformas de streaming, como Netflix e YouTube. Algoritmos como o HEVC (\emph{High Efficiency Video Coding}), que é uma evolução do H.264, estão sendo aprimorados continuamente para garantir uma compressão mais eficiente, sem perda significativa de qualidade. Além disso, o desenvolvimento de novos codecs como o VVC (\emph{Versatile Video Coding}) está sendo acelerado, com a promessa de compressão ainda mais eficiente para vídeos de alta definição e 4K.

%A análise de desempenho, incluindo sua eficiência em termos de tempo de execução e taxa de compressão, também é um tópico relevante. Com o aumento exponencial da quantidade de dados gerados e transmitidos na sociedade moderna, os algoritmos se tornaram ainda mais essenciais. A eficiência na compressão não só reduz custos de armazenamento e acelera a transmissão, mas também tem impacto direto em áreas como streaming de vídeos, comunicação móvel, redes de computadores e sistemas de armazenamento em nuvem. A compreensão dos diferentes métodos e a avaliação de sua aplicabilidade em diferentes cenários são fundamentais para o desenvolvimento de soluções mais eficazes e eficientes.


O tema deste trabalho é a exploração dos algoritmos clássicos de compressão de dados, abordando seus aspectos teóricos, implementação prática e análise de desempenho. A compressão de dados é uma técnica essencial na ciência da computação, utilizada para reduzir o tamanho dos arquivos, otimizando o uso de recursos e acelerando a transmissão de dados, o que se torna crucial na era digital em que vivemos.

O desenvolvimento dessa área teve início na década de 1950, com os primeiros métodos que visavam otimizar a utilização do espaço de armazenamento. Entre esses métodos, destaca-se o algoritmo de Huffman, que utiliza uma técnica de codificação de prefixo variável. Nesse algoritmo, símbolos mais frequentes recebem códigos de comprimento menor, enquanto símbolos menos frequentes recebem códigos mais longos. O método de Huffman é amplamente empregado em compressão sem perdas e se tornou fundamental para a criação de arquivos compactados em formatos como ZIP e GZIP. Sua simplicidade, aliada à sua eficácia, fez com que se consolidasse como um dos pilares da compressão de dados.

Nos anos seguintes, com o aumento da demanda por transmissão de grandes volumes de dados, novos algoritmos surgiram. Um exemplo significativo é o LZ77 (Lempel-Ziv 77), introduzido por Abraham Lempel e Jacob Ziv em 1977. O LZ77 aplica uma técnica de compressão baseada em dicionários, onde sequências de dados repetidas são substituídas por referências a posições anteriores. Essa abordagem inspirou a criação de outros algoritmos, como o LZW (Lempel-Ziv-Welch), que aprimora o LZ77 ao criar dinamicamente um dicionário de strings enquanto os dados são comprimidos. O LZW é particularmente famoso pela sua aplicação no formato de compressão GIF, bem como em arquivos TIFF.

Além desses, o GZIP se destaca como outro algoritmo amplamente utilizado, especialmente em sistemas Unix e na web. O GZIP combina a codificação de Huffman com o algoritmo LZ77, permitindo uma compressão eficiente e rápida, sem perda de dados. Sua popularidade decorre da combinação de alta taxa de compressão e facilidade de descompressão, sendo uma escolha recorrente em aplicações como transferência de arquivos e armazenamento de dados.

Os métodos de Huffman, LZ77, LZW e GZIP formam a espinha dorsal dos algoritmos clássicos de compressão de dados e continuam sendo amplamente utilizados em diversas áreas, apesar do surgimento de novas abordagens.

Atualmente, a compressão de dados continua a ser um campo dinâmico e em constante evolução, com diversos algoritmos sendo constantemente estudados e aprimorados. Entre os mais recentes, destacam-se os métodos baseados em compressão adaptativa, que ajustam seus parâmetros conforme as características dos dados a serem comprimidos, oferecendo maior eficiência dependendo do conteúdo. Um exemplo é o Brotli, um algoritmo de compressão sem perdas desenvolvido pela Google, amplamente utilizado para comprimir arquivos em navegadores web. O Brotli combina codificação de Huffman e transformações de fluxo de dados, permitindo taxas de compressão superiores às oferecidas por algoritmos tradicionais como o GZIP.

Além disso, algoritmos de compressão em tempo real têm ganhado relevância devido ao crescente volume de dados gerados em tempo real, como no streaming de vídeo e nas comunicações móveis. O Zstandard (ou Zstd), desenvolvido pelo Facebook, é um exemplo notável. Ele oferece uma excelente combinação entre alta taxa de compressão e velocidade, podendo ser utilizado tanto em compressões em tempo real quanto em grandes volumes de dados. O Zstandard é considerado uma evolução do LZ4, sendo mais eficiente tanto na compressão quanto na descompressão.

Em áreas como compressão de imagens e áudio, técnicas baseadas em aprendizado de máquina e redes neurais também têm atraído atenção. No campo da compressão de imagens, os Modelos Generativos Adversariais (GANs) estão sendo explorados para desenvolver algoritmos que geram representações comprimidas de alta qualidade, mantendo os detalhes visuais. Na compressão de áudio, algoritmos baseados em redes neurais, como o WaveNet, estão sendo experimentados para aprimorar a qualidade da compressão, tanto com perdas quanto sem perdas.

Outro campo de grande importância é a compressão de vídeo, especialmente para plataformas de streaming como Netflix e YouTube. Algoritmos como o HEVC (High Efficiency Video Coding), uma evolução do H.264, continuam a ser aprimorados para garantir uma compressão mais eficiente, sem perda significativa de qualidade. Além disso, o desenvolvimento de novos codecs como o VVC (Versatile Video Coding) está em andamento, com a promessa de oferecer compressão ainda mais eficiente para vídeos de alta definição e 4K.

A análise de desempenho, que inclui a eficiência em termos de tempo de execução e taxa de compressão, também desempenha um papel crucial. Com o aumento exponencial da quantidade de dados gerados e transmitidos na sociedade moderna, os algoritmos de compressão tornaram-se ainda mais essenciais. A eficiência na compressão não só reduz os custos de armazenamento e acelera a transmissão de dados, mas também impacta diretamente áreas como streaming de vídeos, comunicação móvel, redes de computadores e sistemas de armazenamento em nuvem. Compreender os diferentes métodos e avaliar sua aplicabilidade em cenários diversos é fundamental para o desenvolvimento de soluções mais eficazes e eficientes.
